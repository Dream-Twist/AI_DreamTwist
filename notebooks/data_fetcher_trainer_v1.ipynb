{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-lmMliGDL0T"
      },
      "source": [
        "꿈틀에서 생성된 동화를 AI 모델에 학습시키는 코드입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPesbXeTDUZt"
      },
      "source": [
        "# DB에 접근해 동화 내용 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "u23EDVuA8qV8"
      },
      "outputs": [],
      "source": [
        "import pymysql\n",
        "import json\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "DB_HOST = os.environ.get('DB_HOST')\n",
        "DB_USERNAME = os.environ.get('DB_USERNAME')\n",
        "DB_PASSWORD = os.environ.get('DB_PASSWORD')\n",
        "DB_DATABASE = os.environ.get('DB_DATABASE')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "42hhgZOD8IVm"
      },
      "outputs": [],
      "source": [
        "conn = pymysql.connect(host = DB_HOST, user = DB_USERNAME, password = DB_PASSWORD, db = DB_DATABASE, charset = 'utf8')\n",
        "\n",
        "try:\n",
        "  with conn.cursor() as cursor:\n",
        "    sql = \"SELECT content FROM fairytale WHERE created_at BETWEEN '2024-07-01 00:00:00' AND '2024-08-03 23:59:59';\" # 원하는 기간 설정\n",
        "    cursor.execute(sql)\n",
        "    result = cursor.fetchall()\n",
        "\n",
        "    train_texts = []\n",
        "    for datas in result:\n",
        "      dict = json.loads(datas[0])\n",
        "      fairy = ''\n",
        "      cnt = 0\n",
        "      for fairytale in dict.values():\n",
        "        cnt += 1\n",
        "        fairy += f\"{fairytale} \"\n",
        "        if cnt >= len(dict.values()):\n",
        "          train_texts.append(fairy.rstrip())\n",
        "except:\n",
        "  print('예외가 발생했습니다.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkZ6D_6XBa8l"
      },
      "source": [
        "# 슬라이딩 윈도우 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq8G3b1AA_gv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import re\n",
        "import warnings\n",
        "import random\n",
        "warnings.filterwarnings('ignore')\n",
        "from datasets import Dataset, DatasetDict\n",
        "from torch.utils.data import Dataset as TorchDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b7FsxtrBdfj"
      },
      "outputs": [],
      "source": [
        "# 슬라이딩 윈도우 파라미터\n",
        "window_size = 10  # 윈도우 크기 (문장 수)\n",
        "sliding_step = 5  # 슬라이딩 간격 (문장 수)\n",
        "\n",
        "def sliding_window(sentences, window_size, step):\n",
        "    \"\"\" 문장 리스트에 슬라이딩 윈도우를 적용하는 함수 \"\"\"\n",
        "    windows = []\n",
        "    for start in range(0, len(sentences) - window_size + 1, step):\n",
        "        window = sentences[start:start + window_size]\n",
        "        windows.append(window)\n",
        "\n",
        "    # 마지막 윈도우 추가 (중복 방지)\n",
        "    if len(sentences) % step != 0:\n",
        "        last_window = sentences[-window_size:]\n",
        "        if last_window not in windows:\n",
        "            windows.append(last_window)\n",
        "    return windows\n",
        "\n",
        "\n",
        "# 동화 데이터에 슬라이딩 윈도우 적용\n",
        "all_story_windows = []\n",
        "\n",
        "for story in train_texts:\n",
        "    # 동화에서 문장 분리\n",
        "    sentences = sent_tokenize(story)\n",
        "\n",
        "    # 슬라이딩 윈도우 적용\n",
        "    story_windows = sliding_window(sentences, window_size, sliding_step)\n",
        "\n",
        "    all_story_windows.append(story_windows)\n",
        "\n",
        "# 문장 수가 충분하지 않은 경우를 확인\n",
        "for i, story in enumerate(train_texts):\n",
        "    sentences = sent_tokenize(story)\n",
        "    if len(sentences) < window_size:\n",
        "        print(f\"Story {i} has less than {window_size} sentences and cannot have any windows.\")\n",
        "\n",
        "all_new_story = []\n",
        "\n",
        "for story_window in all_story_windows:\n",
        "    sentences_list = []\n",
        "\n",
        "    for sentences in story_window:\n",
        "        sentences_list.extend(sentences)\n",
        "\n",
        "    new_story = ' '.join(sentences_list)\n",
        "    new_story = new_story.replace('\\n', ' ').strip()\n",
        "\n",
        "    all_new_story.append(new_story)\n",
        "\n",
        "train_texts = all_new_story"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqIVbLyQBmHb"
      },
      "source": [
        "# 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q3_m9uuBm2b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import Trainer, TrainingArguments, GPT2LMHeadModel, PreTrainedTokenizerFast, DataCollatorForLanguageModeling\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "# GPU 환경에서 사용 가능하도록 변경\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 학습시킨 꿈틀 AI 모델\n",
        "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "model_path = os.path.join(current_dir, '../models/story_generator')\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "\n",
        "# 토크나이저 로드\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "tokenizer.add_special_tokens({'bos_token': '<BOS>'})\n",
        "tokenizer.add_special_tokens({'eos_token': '<EOS>'})\n",
        "tokenizer.add_special_tokens({'sep_token': '<SEP>'})\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# 꿈틀 데이터셋 클래스 정의\n",
        "class DreamTwistDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length):\n",
        "        self.encodings = tokenizer(texts, padding=True, truncation=True, max_length=max_length)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]).to(device) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "# 데이터셋 인스턴스 생성\n",
        "max_length = 512\n",
        "\n",
        "# 꿈틀 트레인 데이터셋 설정\n",
        "train_dataset = DreamTwistDataset(train_texts, tokenizer, max_length)\n",
        "\n",
        "# 데이터 콜레이터 설정\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        "    pad_to_multiple_of=max_length\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=50 ,\n",
        "    per_device_train_batch_size=1,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    eval_strategy='epoch',\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    logging_first_step=True,\n",
        "    learning_rate=5e-5,\n",
        "    overwrite_output_dir=True,\n",
        ")\n",
        "\n",
        "# 트레이너 객체 생성 및 훈련\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=train_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbJwSOwuBwfe"
      },
      "source": [
        "모델 내보내기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UE_1TToTBrqh"
      },
      "outputs": [],
      "source": [
        "output_dir = './trained_model/new_model'\n",
        "# 모델 저장\n",
        "model.save_pretrained(output_dir)\n",
        "# 토크나이저 저장\n",
        "tokenizer.save_pretrained(output_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
